{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f276bf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40414484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Loading blaze999/Medical-NER...\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mðŸ“¥ Loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m model = \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m(model_name)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m80\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMODEL CONFIGURATION DETAILS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajilj\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2157\u001b[39m, in \u001b[36mDummyObject.__getattribute__\u001b[39m\u001b[34m(cls, key)\u001b[39m\n\u001b[32m   2155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (key.startswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key != \u001b[33m\"\u001b[39m\u001b[33m_from_config\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mis_dummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mmro\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key == \u001b[33m\"\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m2157\u001b[39m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ajilj\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\transformers\\utils\\import_utils.py:2143\u001b[39m, in \u001b[36mrequires_backends\u001b[39m\u001b[34m(obj, backends)\u001b[39m\n\u001b[32m   2140\u001b[39m         failed.append(msg.format(name))\n\u001b[32m   2142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[32m-> \u001b[39m\u001b[32m2143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(failed))\n",
      "\u001b[31mImportError\u001b[39m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment. Check out the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\nPlease note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "# Load the model\n",
    "model_name = \"blaze999/Medical-NER\"\n",
    "print(f\"ðŸ“¥ Loading {model_name}...\\n\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL CONFIGURATION DETAILS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =====================================================\n",
    "# 1ï¸âƒ£ BASIC MODEL INFO\n",
    "# =====================================================\n",
    "print(\"\\nðŸ“‹ BASIC MODEL INFORMATION:\")\n",
    "print(f\"   Model Name: {model_name}\")\n",
    "print(f\"   Architecture: {model.config.architectures}\")\n",
    "print(f\"   Num Labels: {model.config.num_labels}\")\n",
    "print(f\"   Hidden Size: {model.config.hidden_size}\")\n",
    "print(f\"   Num Attention Heads: {model.config.num_attention_heads}\")\n",
    "print(f\"   Num Hidden Layers: {model.config.num_hidden_layers}\")\n",
    "print(f\"   Vocab Size: {model.config.vocab_size}\")\n",
    "\n",
    "# =====================================================\n",
    "# 2ï¸âƒ£ LABEL MAPPINGS (ID TO LABEL & LABEL TO ID)\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LABEL MAPPINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "id2label = model.config.id2label\n",
    "label2id = model.config.label2id\n",
    "\n",
    "print(f\"\\nðŸ“Š Total Labels: {len(id2label)}\")\n",
    "print(\"\\nðŸ”¢ ID â†’ LABEL Mapping:\")\n",
    "for label_id, label_name in sorted(id2label.items()):\n",
    "    print(f\"   {label_id:3d} â†’ {label_name}\")\n",
    "\n",
    "print(\"\\nðŸ·ï¸  LABEL â†’ ID Mapping:\")\n",
    "for label_name, label_id in sorted(label2id.items()):\n",
    "    print(f\"   {label_name:30s} â†’ {label_id}\")\n",
    "\n",
    "# =====================================================\n",
    "# 3ï¸âƒ£ ENTITY TYPES (EXTRACTED FROM LABELS)\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENTITY TYPES (BIO TAGGING SCHEME)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "entity_types = {}\n",
    "for label in id2label.values():\n",
    "    if label.startswith('B-'):\n",
    "        entity_type = label[2:]\n",
    "        if entity_type not in entity_types:\n",
    "            entity_types[entity_type] = {'B': [], 'I': []}\n",
    "        entity_types[entity_type]['B'].append(label)\n",
    "    elif label.startswith('I-'):\n",
    "        entity_type = label[2:]\n",
    "        if entity_type not in entity_types:\n",
    "            entity_types[entity_type] = {'B': [], 'I': []}\n",
    "        entity_types[entity_type]['I'].append(label)\n",
    "    elif label != 'O':\n",
    "        if label not in entity_types:\n",
    "            entity_types[label] = {'B': [], 'I': []}\n",
    "\n",
    "print(f\"\\nðŸ“Œ Total Entity Types: {len(entity_types)}\")\n",
    "print(\"\\nðŸ¥ Entity Types with BIO Tags:\")\n",
    "for i, (entity_type, tags) in enumerate(sorted(entity_types.items()), 1):\n",
    "    b_tags = tags.get('B', [])\n",
    "    i_tags = tags.get('I', [])\n",
    "    print(f\"\\n   {i:2d}. {entity_type}\")\n",
    "    if b_tags:\n",
    "        print(f\"       B-tags: {', '.join(b_tags)}\")\n",
    "    if i_tags:\n",
    "        print(f\"       I-tags: {', '.join(i_tags)}\")\n",
    "\n",
    "# =====================================================\n",
    "# 4ï¸âƒ£ JSON FORMAT OUTPUT\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"JSON FORMAT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "json_output = {\n",
    "    \"model_info\": {\n",
    "        \"name\": model_name,\n",
    "        \"architecture\": model.config.architectures,\n",
    "        \"num_labels\": model.config.num_labels,\n",
    "        \"hidden_size\": model.config.hidden_size,\n",
    "        \"num_attention_heads\": model.config.num_attention_heads,\n",
    "        \"num_hidden_layers\": model.config.num_hidden_layers,\n",
    "        \"vocab_size\": model.config.vocab_size\n",
    "    },\n",
    "    \"label_mappings\": {\n",
    "        \"id2label\": id2label,\n",
    "        \"label2id\": label2id\n",
    "    },\n",
    "    \"entity_types\": {\n",
    "        entity_type: {\n",
    "            \"b_tags\": tags.get('B', []),\n",
    "            \"i_tags\": tags.get('I', [])\n",
    "        }\n",
    "        for entity_type, tags in sorted(entity_types.items())\n",
    "    },\n",
    "    \"bio_scheme\": {\n",
    "        \"description\": \"BIO (Begin-Inside-Outside) tagging scheme\",\n",
    "        \"tags\": {\n",
    "            \"B-\": \"Beginning of entity\",\n",
    "            \"I-\": \"Inside entity (continuation)\",\n",
    "            \"O\": \"Outside any entity\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“„ JSON Output:\")\n",
    "print(json.dumps(json_output, indent=2, ensure_ascii=False))\n",
    "\n",
    "# Save JSON to file\n",
    "with open(\"model_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_output, f, indent=2, ensure_ascii=False)\n",
    "print(\"\\nâœ… JSON saved to: model_details.json\")\n",
    "\n",
    "# =====================================================\n",
    "# 5ï¸âƒ£ XML FORMAT OUTPUT\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"XML FORMAT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create XML structure\n",
    "root = ET.Element(\"medical_ner_model\")\n",
    "\n",
    "# Model Info\n",
    "model_info_elem = ET.SubElement(root, \"model_info\")\n",
    "ET.SubElement(model_info_elem, \"name\").text = model_name\n",
    "ET.SubElement(model_info_elem, \"architecture\").text = str(model.config.architectures)\n",
    "ET.SubElement(model_info_elem, \"num_labels\").text = str(model.config.num_labels)\n",
    "ET.SubElement(model_info_elem, \"hidden_size\").text = str(model.config.hidden_size)\n",
    "ET.SubElement(model_info_elem, \"num_attention_heads\").text = str(model.config.num_attention_heads)\n",
    "ET.SubElement(model_info_elem, \"num_hidden_layers\").text = str(model.config.num_hidden_layers)\n",
    "ET.SubElement(model_info_elem, \"vocab_size\").text = str(model.config.vocab_size)\n",
    "\n",
    "# Label Mappings\n",
    "label_mappings_elem = ET.SubElement(root, \"label_mappings\")\n",
    "\n",
    "# ID to Label\n",
    "id2label_elem = ET.SubElement(label_mappings_elem, \"id2label\")\n",
    "for label_id, label_name in sorted(id2label.items()):\n",
    "    label_elem = ET.SubElement(id2label_elem, \"label\", id=str(label_id))\n",
    "    label_elem.text = label_name\n",
    "\n",
    "# Label to ID\n",
    "label2id_elem = ET.SubElement(label_mappings_elem, \"label2id\")\n",
    "for label_name, label_id in sorted(label2id.items()):\n",
    "    label_elem = ET.SubElement(label2id_elem, \"label\", name=label_name)\n",
    "    label_elem.text = str(label_id)\n",
    "\n",
    "# Entity Types\n",
    "entity_types_elem = ET.SubElement(root, \"entity_types\")\n",
    "for entity_type, tags in sorted(entity_types.items()):\n",
    "    entity_elem = ET.SubElement(entity_types_elem, \"entity\", type=entity_type)\n",
    "    \n",
    "    if tags.get('B'):\n",
    "        b_tags_elem = ET.SubElement(entity_elem, \"b_tags\")\n",
    "        for tag in tags['B']:\n",
    "            ET.SubElement(b_tags_elem, \"tag\").text = tag\n",
    "    \n",
    "    if tags.get('I'):\n",
    "        i_tags_elem = ET.SubElement(entity_elem, \"i_tags\")\n",
    "        for tag in tags['I']:\n",
    "            ET.SubElement(i_tags_elem, \"tag\").text = tag\n",
    "\n",
    "# BIO Scheme Description\n",
    "bio_scheme_elem = ET.SubElement(root, \"bio_scheme\")\n",
    "ET.SubElement(bio_scheme_elem, \"description\").text = \"BIO (Begin-Inside-Outside) tagging scheme\"\n",
    "tags_elem = ET.SubElement(bio_scheme_elem, \"tags\")\n",
    "ET.SubElement(tags_elem, \"B\", description=\"Beginning of entity\")\n",
    "ET.SubElement(tags_elem, \"I\", description=\"Inside entity (continuation)\")\n",
    "ET.SubElement(tags_elem, \"O\", description=\"Outside any entity\")\n",
    "\n",
    "# Pretty print XML\n",
    "xml_string = minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"  \")\n",
    "\n",
    "print(\"\\nðŸ“„ XML Output:\")\n",
    "print(xml_string)\n",
    "\n",
    "# Save XML to file\n",
    "with open(\"model_details.xml\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(xml_string)\n",
    "print(\"\\nâœ… XML saved to: model_details.xml\")\n",
    "\n",
    "# =====================================================\n",
    "# 6ï¸âƒ£ TOKENIZER DETAILS\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOKENIZER DETAILS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nðŸ”¤ Tokenizer Type: {type(tokenizer).__name__}\")\n",
    "print(f\"   Vocab Size: {len(tokenizer)}\")\n",
    "print(f\"   Model Max Length: {tokenizer.model_max_length}\")\n",
    "print(f\"   Padding Side: {tokenizer.padding_side}\")\n",
    "print(f\"   Truncation Side: {tokenizer.truncation_side}\")\n",
    "\n",
    "# Special tokens\n",
    "print(f\"\\n   Special Tokens:\")\n",
    "print(f\"      PAD: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})\")\n",
    "print(f\"      UNK: {tokenizer.unk_token} (id: {tokenizer.unk_token_id})\")\n",
    "print(f\"      CLS: {tokenizer.cls_token} (id: {tokenizer.cls_token_id})\")\n",
    "print(f\"      SEP: {tokenizer.sep_token} (id: {tokenizer.sep_token_id})\")\n",
    "print(f\"      MASK: {tokenizer.mask_token} (id: {tokenizer.mask_token_id})\")\n",
    "\n",
    "# =====================================================\n",
    "# 7ï¸âƒ£ SAMPLE ENTITY GROUPINGS FOR YOUR USE\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDED ENTITY GROUPINGS FOR MEDICAL APPLICATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "entity_groupings = {\n",
    "    \"MEDICATIONS\": [\"MEDICATION\"],\n",
    "    \"SYMPTOMS_AND_CONDITIONS\": [\"SIGN_SYMPTOM\", \"DISEASE_DISORDER\", \"OUTCOME\"],\n",
    "    \"PROCEDURES\": [\"DIAGNOSTIC_PROCEDURE\", \"THERAPEUTIC_PROCEDURE\"],\n",
    "    \"TEMPORAL_INFO\": [\"DURATION\", \"FREQUENCY\", \"DATE\", \"TIME\", \"AGE\"],\n",
    "    \"DOSAGE_INFO\": [\"DOSAGE\", \"ADMINISTRATION\"],\n",
    "    \"MEASUREMENTS\": [\"LAB_VALUE\", \"MASS\", \"HEIGHT\", \"WEIGHT\", \"VOLUME\", \"DISTANCE\"],\n",
    "    \"ANATOMICAL\": [\"BIOLOGICAL_STRUCTURE\", \"AREA\"],\n",
    "    \"PATIENT_INFO\": [\"SEX\", \"AGE\", \"FAMILY_HISTORY\", \"PERSONAL_BACKGROUND\", \"OCCUPATION\"],\n",
    "    \"CLINICAL_EVENTS\": [\"CLINICAL_EVENT\", \"OTHER_EVENT\", \"ACTIVITY\"],\n",
    "    \"DESCRIPTIVE\": [\"SEVERITY\", \"COLOR\", \"SHAPE\", \"TEXTURE\", \"DETAILED_DESCRIPTION\", \n",
    "                    \"BIOLOGICAL_ATTRIBUTE\", \"QUALITATIVE_CONCEPT\", \"QUANTITATIVE_CONCEPT\"]\n",
    "}\n",
    "\n",
    "print(\"\\nðŸ“Š Suggested entity groupings for your application:\")\n",
    "for group_name, entities in entity_groupings.items():\n",
    "    print(f\"\\n   {group_name}:\")\n",
    "    for entity in entities:\n",
    "        if entity in entity_types:\n",
    "            print(f\"      âœ“ {entity}\")\n",
    "        else:\n",
    "            print(f\"      âœ— {entity} (not in model)\")\n",
    "\n",
    "# Save groupings as JSON\n",
    "with open(\"entity_groupings.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(entity_groupings, f, indent=2)\n",
    "print(\"\\nâœ… Entity groupings saved to: entity_groupings.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… COMPLETE! All details exported to:\")\n",
    "print(\"   - model_details.json\")\n",
    "print(\"   - model_details.xml\")\n",
    "print(\"   - entity_groupings.json\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68d220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8d16a51",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca6904ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.9.0-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\ajilj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\ajilj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (4.15.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\ajilj\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch) (2025.10.0)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached markupsafe-3.0.3-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
      "Downloading torch-2.9.0-cp313-cp313-win_amd64.whl (109.3 MB)\n",
      "   ---------------------------------------- 0.0/109.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.4/109.3 MB 17.3 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 4.7/109.3 MB 13.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 5.8/109.3 MB 10.3 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 7.1/109.3 MB 9.4 MB/s eta 0:00:11\n",
      "   --- ------------------------------------ 8.4/109.3 MB 9.0 MB/s eta 0:00:12\n",
      "   --- ------------------------------------ 8.9/109.3 MB 8.2 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 9.4/109.3 MB 7.1 MB/s eta 0:00:15\n",
      "   --- ------------------------------------ 9.7/109.3 MB 6.6 MB/s eta 0:00:16\n",
      "   --- ------------------------------------ 10.2/109.3 MB 6.0 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 10.7/109.3 MB 5.6 MB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 11.5/109.3 MB 5.3 MB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 12.1/109.3 MB 5.1 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 12.8/109.3 MB 4.9 MB/s eta 0:00:20\n",
      "   ---- ----------------------------------- 13.4/109.3 MB 4.8 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 14.2/109.3 MB 4.7 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 14.9/109.3 MB 4.6 MB/s eta 0:00:21\n",
      "   ----- ---------------------------------- 15.7/109.3 MB 4.6 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 16.5/109.3 MB 4.5 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 17.3/109.3 MB 4.5 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 18.1/109.3 MB 4.5 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 19.1/109.3 MB 4.5 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 19.9/109.3 MB 4.5 MB/s eta 0:00:21\n",
      "   ------- -------------------------------- 21.0/109.3 MB 4.5 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 22.0/109.3 MB 4.5 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 23.1/109.3 MB 4.5 MB/s eta 0:00:20\n",
      "   -------- ------------------------------- 24.4/109.3 MB 4.6 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 25.4/109.3 MB 4.6 MB/s eta 0:00:19\n",
      "   --------- ------------------------------ 26.5/109.3 MB 4.6 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 27.8/109.3 MB 4.7 MB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 29.1/109.3 MB 4.7 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 30.4/109.3 MB 4.8 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 31.7/109.3 MB 4.8 MB/s eta 0:00:17\n",
      "   ------------ --------------------------- 33.0/109.3 MB 4.9 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 34.3/109.3 MB 4.9 MB/s eta 0:00:16\n",
      "   ------------- -------------------------- 35.9/109.3 MB 5.0 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 37.2/109.3 MB 5.0 MB/s eta 0:00:15\n",
      "   -------------- ------------------------- 38.8/109.3 MB 5.1 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 40.4/109.3 MB 5.2 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 41.9/109.3 MB 5.2 MB/s eta 0:00:13\n",
      "   --------------- ------------------------ 43.5/109.3 MB 5.3 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 44.8/109.3 MB 5.3 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 46.1/109.3 MB 5.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 47.2/109.3 MB 5.3 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 48.2/109.3 MB 5.3 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 49.5/109.3 MB 5.4 MB/s eta 0:00:12\n",
      "   ------------------ --------------------- 50.9/109.3 MB 5.4 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 52.2/109.3 MB 5.4 MB/s eta 0:00:11\n",
      "   ------------------- -------------------- 53.5/109.3 MB 5.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 54.8/109.3 MB 5.4 MB/s eta 0:00:11\n",
      "   -------------------- ------------------- 56.1/109.3 MB 5.5 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 57.7/109.3 MB 5.5 MB/s eta 0:00:10\n",
      "   --------------------- ------------------ 59.2/109.3 MB 5.5 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 60.6/109.3 MB 5.6 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 61.9/109.3 MB 5.6 MB/s eta 0:00:09\n",
      "   ---------------------- ----------------- 62.7/109.3 MB 5.6 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 63.4/109.3 MB 5.5 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 64.2/109.3 MB 5.5 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 65.3/109.3 MB 5.5 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 66.1/109.3 MB 5.4 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 66.8/109.3 MB 5.4 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 67.9/109.3 MB 5.4 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 68.9/109.3 MB 5.4 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 70.0/109.3 MB 5.4 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 71.0/109.3 MB 5.4 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 72.1/109.3 MB 5.4 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 73.1/109.3 MB 5.4 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 74.4/109.3 MB 5.4 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 75.5/109.3 MB 5.4 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 76.8/109.3 MB 5.4 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 78.1/109.3 MB 5.4 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 79.2/109.3 MB 5.4 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 80.5/109.3 MB 5.4 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 81.8/109.3 MB 5.5 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 83.4/109.3 MB 5.5 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 84.7/109.3 MB 5.5 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 86.2/109.3 MB 5.5 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 87.8/109.3 MB 5.5 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 89.1/109.3 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 90.7/109.3 MB 5.6 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 92.3/109.3 MB 5.6 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 93.6/109.3 MB 5.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 94.4/109.3 MB 5.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 95.4/109.3 MB 5.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 96.2/109.3 MB 5.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 97.0/109.3 MB 5.6 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 98.0/109.3 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 98.8/109.3 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 99.9/109.3 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 100.9/109.3 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 101.7/109.3 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 103.0/109.3 MB 5.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 104.1/109.3 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 105.4/109.3 MB 5.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 106.4/109.3 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  107.7/109.3 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  109.1/109.3 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 109.3/109.3 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 1.0/2.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 5.9 MB/s eta 0:00:00\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached markupsafe-3.0.3-cp313-cp313-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, setuptools, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.3 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 setuptools-80.9.0 sympy-1.14.0 torch-2.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install torch \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7023e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
